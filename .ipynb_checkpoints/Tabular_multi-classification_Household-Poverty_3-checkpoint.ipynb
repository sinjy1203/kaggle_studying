{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0f8e3b",
   "metadata": {},
   "source": [
    "## LGBM with random split for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b6f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24aa3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_data(df):\n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
    "    \n",
    "def feature_importance(forest, X_train, display_results=True):\n",
    "    ranked_list = []\n",
    "    zero_features = []\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    if display_results:\n",
    "        print('Feature ranking:')\n",
    "        \n",
    "    for f in range(X_train.shape[1]):\n",
    "        if display_results:\n",
    "            print('%d. feature %d (%f)' % (f+1, indices[f], importances[indices[f]]) + ' - ' + X_train.columns[indices[f]])\n",
    "        ranked_list.append(X_train.columns[indices[f]])\n",
    "        if importances[indices[f]] == 0.0:\n",
    "            zero_features.append(X_train.columns[indices[f]])\n",
    "    \n",
    "    return ranked_list, zero_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6fbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n",
    "                ]\n",
    "    \n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "    \n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)\n",
    "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "        \n",
    "    aggs_num = {'age': ['min', 'max', 'mean'], \n",
    "               'escolari': ['min', 'max', 'mean']}\n",
    "    \n",
    "    aggs_cat = {'dis': ['mean']}\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean', 'count']\n",
    "    \n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + '_' + e[1].upper() for e in df_agg.columns.to_list()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "    \n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b388bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n",
    "               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n",
    "               'instlevel', 'lugar', 'tipovivi',\n",
    "               'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        \n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'.format(s_))\n",
    "            col_dummy = s_ + '_dummy'\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            \n",
    "            cols_s_.append(col_dummy)\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                print(\"The category completion did not work\")\n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23271d30",
   "metadata": {},
   "source": [
    "## Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "babc9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_dir = Path('C:/Users/sinjy/jupyter_notebook/datasets/kaggle_datasets/Household-Poverty_dataset')\n",
    "train = pd.read_csv(data_dir / 'train.csv')\n",
    "test = pd.read_csv(data_dir / 'test.csv')\n",
    "\n",
    "test_ids = test.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e7e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    encode_data(df_)\n",
    "    return do_features(df_)\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31d311e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
    "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
    "\n",
    "train.loc[train['edjefa'] == 'no', 'edjefa'] = 0\n",
    "train.loc[train['edjefe'] == 'no', 'edjefe'] = 0\n",
    "test.loc[test['edjefa'] == 'no', 'edjefa'] = 0\n",
    "test.loc[test['edjefe'] == 'no', 'edjefe'] = 0\n",
    "\n",
    "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "train.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "train.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "test.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "test.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "train['edjefe'] = train['edjefe'].astype(\"int\")\n",
    "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
    "test['edjefe'] = test['edjefe'].astype(\"int\")\n",
    "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
    "\n",
    "train['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\n",
    "test['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n",
    "\n",
    "train['v2a1']=train['v2a1'].fillna(0)\n",
    "test['v2a1']=test['v2a1'].fillna(0)\n",
    "\n",
    "test['v18q1']=test['v18q1'].fillna(0)\n",
    "train['v18q1']=train['v18q1'].fillna(0)\n",
    "\n",
    "train['rez_esc']=train['rez_esc'].fillna(0)\n",
    "test['rez_esc']=test['rez_esc'].fillna(0)\n",
    "\n",
    "train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "test.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n",
    "\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bdea2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "    \n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_ = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n",
    "    \n",
    "    del xx, xx_func\n",
    "    return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50be5f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OHE in techo is incomplete. A new column will be added before label encoding\n",
      "The OHE in instlevel is incomplete. A new column will be added before label encoding\n",
      "The OHE in manual_elec is incomplete. A new column will be added before label encoding\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98955396",
   "metadata": {},
   "source": [
    "## Geo aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d9d05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
    "              'pared_LE']\n",
    "cols_nums = ['age', 'meaneduc', 'dependency', \n",
    "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
    "             'bedrooms', 'overcrowding']\n",
    "\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar'] + cols_nums)], \n",
    "                       pd.get_dummies(df_[cols_2_ohe], columns=cols_2_ohe)], \n",
    "                      axis=1)\n",
    "    geo_agg = tmp_df.groupby(['lugar_LE', 'idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
    "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.to_list()])\n",
    "    \n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
    "\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a951b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['num_over_18'] = 0\n",
    "train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\").iloc[:, 0]\n",
    "train['num_over_18'] = train.groupby('idhogar')['num_over_18'].transform('max')\n",
    "train['num_over_18'] = train['num_over_18'].fillna(0)\n",
    "\n",
    "test['num_over_18'] = 0\n",
    "test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\").iloc[:, 0]\n",
    "test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "test['num_over_18'] = test['num_over_18'].fillna(0)\n",
    "\n",
    "def extract_features(df):\n",
    "    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n",
    "    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n",
    "    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n",
    "    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n",
    "    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent to people in household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n",
    "    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # rooms per person\n",
    "    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent to household size\n",
    "    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n",
    "    # some households have no one over 18, use the total rent for those\n",
    "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n",
    "\n",
    "extract_features(train)\n",
    "extract_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a012266",
   "metadata": {},
   "outputs": [],
   "source": [
    "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n",
    "                 'mobilephone', 'female']\n",
    "instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n",
    "\n",
    "needless_cols.extend(instlevel_cols)\n",
    "\n",
    "train = train.drop(needless_cols, axis=1)\n",
    "test = test.drop(needless_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e496a",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7528d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(train, y, sample_weight=None, households=None, \n",
    "              test_percentage=0.2, seed=None):\n",
    "    train2 = train.copy()\n",
    "    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n",
    "    \n",
    "    cv_idx = np.isin(households, cv_hhs)\n",
    "    X_test = train2[cv_idx]\n",
    "    y_test = y[cv_idx]\n",
    "    \n",
    "    X_train = train2[~cv_idx]\n",
    "    y_train = y[~cv_idx]\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        y_train_weights = sample_weight[~cv_idx]\n",
    "        return X_train, y_train, X_test, y_test, y_train_weights\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6867d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.query('parentesco1==1')\n",
    "\n",
    "y = X['Target'] - 1\n",
    "X = X.drop(['Target'], axis=1)\n",
    "\n",
    "np.random.seed(seed=None)\n",
    "\n",
    "train2 = X.copy()\n",
    "\n",
    "train_hhs = train2.idhogar\n",
    "\n",
    "households = train2.idhogar.unique()\n",
    "cv_hhs = np.random.choice(households, size=int(len(households)*0.15), replace=False)\n",
    "\n",
    "cv_idx = np.isin(train2.idhogar, cv_hhs)\n",
    "\n",
    "X_test = train2[cv_idx]\n",
    "y_test = y[cv_idx]\n",
    "\n",
    "X_train = train2[~cv_idx]\n",
    "y_train = y[~cv_idx]\n",
    "\n",
    "X_train = train2\n",
    "y_train = y\n",
    "\n",
    "train_households = X_train.idhogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33219d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0837d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_drop_features = [\n",
    " 'agg18_estadocivil1_MEAN',\n",
    " 'agg18_estadocivil6_COUNT',\n",
    " 'agg18_estadocivil7_COUNT',\n",
    " 'agg18_parentesco10_COUNT',\n",
    " 'agg18_parentesco11_COUNT',\n",
    " 'agg18_parentesco12_COUNT',\n",
    " 'agg18_parentesco1_COUNT',\n",
    " 'agg18_parentesco2_COUNT',\n",
    " 'agg18_parentesco3_COUNT',\n",
    " 'agg18_parentesco4_COUNT',\n",
    " 'agg18_parentesco5_COUNT',\n",
    " 'agg18_parentesco6_COUNT',\n",
    " 'agg18_parentesco7_COUNT',\n",
    " 'agg18_parentesco8_COUNT',\n",
    " 'agg18_parentesco9_COUNT',\n",
    " 'geo_elimbasu_LE_4',\n",
    " 'geo_energcocinar_LE_1',\n",
    " 'geo_energcocinar_LE_2',\n",
    " 'geo_epared_LE_0',\n",
    " 'geo_hogar_mayor',\n",
    " 'geo_manual_elec_LE_2',\n",
    " 'geo_pared_LE_3',\n",
    " 'geo_pared_LE_4',\n",
    " 'geo_pared_LE_5',\n",
    " 'geo_pared_LE_6',\n",
    " 'num_over_18',\n",
    " 'parentesco_LE',\n",
    " 'rez_esc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e14a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_drop_cols = extra_drop_features + ['idhogar', 'parentesco1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aacbc1",
   "metadata": {},
   "source": [
    "## Fit a voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c2a3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_parameters = {\n",
    "    'max_depth':35, 'eta':0.15, 'silent':1, 'objective':'multi:softmax', \n",
    "    'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, \n",
    "    'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, \n",
    "    'reg_lambda': 0.35 }\n",
    "\n",
    "def evaluate_macroF1_lgb(predictions, truth):\n",
    "    pred_labels = predictions.argmax(axis=1)\n",
    "    truth = truth.get_label()\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', 1-f1)\n",
    "\n",
    "fit_params = {\n",
    "    'early_stopping_rounds': 500,\n",
    "    'eval_metric': evaluate_macroF1_lgb,\n",
    "    'eval_set': [(X_train, y_train), (X_test, y_test)],\n",
    "    'verbose': False}\n",
    "\n",
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate * np.power(.995, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "fit_params['verbose'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0caed565",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "def _parallel_fit_estimator(estimator1, X, y, sample_weight=None, \n",
    "                            threshold=True, **fit_params):\n",
    "    estimator = clone(estimator1)\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        X_train, y_train, X_test, y_test, y_train_weight = split_data(\n",
    "            X, y, sample_weight, households=train_households)\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = split_data(\n",
    "            X, y, None, households=train_households)\n",
    "        \n",
    "    fit_params['eval_set'] = [(X_test, y_test)]\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, \n",
    "                             **fit_params)\n",
    "    else:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, **fit_params)\n",
    "            \n",
    "    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n",
    "        best_cv_round = np.argmax(estimator.eval_result_['validation_0']['mlogloss'])\n",
    "        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n",
    "    else:\n",
    "        best_train = f1_score(y_train, estimator.predict(X_train), average='macro')\n",
    "        best_cv = f1_score(y_test, estimator.predict(X_test), average='macro')\n",
    "        print('Train F1:', best_train)\n",
    "        print('Test F1:', best_cv)\n",
    "    \n",
    "    if threshold:\n",
    "        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n",
    "            return estimator\n",
    "        else:\n",
    "            print('Unacceptable!!! Trying again...')\n",
    "            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n",
    "    else:\n",
    "        return estimator\n",
    "    \n",
    "class VotingClassifierLGBM(VotingClassifier):\n",
    "    def fit(self, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            raise NotImplementedError('Multilabel and multi-output' ' classification is not supported.')\n",
    "        \n",
    "        if self.voting not in ('soft', 'hard'):\n",
    "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\" % self.voting)\n",
    "        \n",
    "        if self.estimators is None or len(self.estimators) == 0:\n",
    "            raise AttributeError(\"Invalid 'estimators' attribute, 'estimators' should be a list of (string, estimator) tuples\")\n",
    "        \n",
    "        if (self.weights is not None and len(self.weights) != len(self.estimators)):\n",
    "            raise ValueError('Number of classifiers and weights must be equal' '; got %d weights, %d estimators' % (len(self.weights), len(self.estimators)))\n",
    "        \n",
    "        names, clfs = zip(*self.estimators)\n",
    "        self._validate_names(names)\n",
    "        \n",
    "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
    "        \n",
    "        if n_isnone == len(self.estimators):\n",
    "            raise ValueError('All estimators are None. At least one is ' 'required to be a classifier')\n",
    "            \n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.estimators_ = []\n",
    "        \n",
    "        transformed_y = self.le_.transform(y)\n",
    "        \n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y, sample_weight=sample_weight, threshold=threshold, **fit_params) for clf in clfs if clf is not None)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57b47774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:22:48] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:22:48] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29846\tvalidation_0-macroF1:0.63511\n",
      "[50]\tvalidation_0-mlogloss:0.92213\tvalidation_0-macroF1:0.58972\n",
      "[100]\tvalidation_0-mlogloss:0.91937\tvalidation_0-macroF1:0.58666\n",
      "[150]\tvalidation_0-mlogloss:0.92056\tvalidation_0-macroF1:0.58155\n",
      "[200]\tvalidation_0-mlogloss:0.91980\tvalidation_0-macroF1:0.57618\n",
      "[250]\tvalidation_0-mlogloss:0.92309\tvalidation_0-macroF1:0.57905\n",
      "[299]\tvalidation_0-mlogloss:0.92146\tvalidation_0-macroF1:0.58622\n",
      "Train F1: 0.8949017334020988\n",
      "Test F1: 0.42648032328678187\n",
      "[11:23:36] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:23:36] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30287\tvalidation_0-macroF1:0.64528\n",
      "[50]\tvalidation_0-mlogloss:0.89817\tvalidation_0-macroF1:0.56289\n",
      "[100]\tvalidation_0-mlogloss:0.89453\tvalidation_0-macroF1:0.57974\n",
      "[150]\tvalidation_0-mlogloss:0.89259\tvalidation_0-macroF1:0.57906\n",
      "[200]\tvalidation_0-mlogloss:0.89393\tvalidation_0-macroF1:0.57148\n",
      "[250]\tvalidation_0-mlogloss:0.89492\tvalidation_0-macroF1:0.56985\n",
      "[299]\tvalidation_0-mlogloss:0.89537\tvalidation_0-macroF1:0.56919\n",
      "Train F1: 0.8995103805102174\n",
      "Test F1: 0.4459881092537203\n",
      "[11:24:23] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:24:23] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30265\tvalidation_0-macroF1:0.63157\n",
      "[50]\tvalidation_0-mlogloss:0.94099\tvalidation_0-macroF1:0.56241\n",
      "[100]\tvalidation_0-mlogloss:0.93632\tvalidation_0-macroF1:0.56762\n",
      "[150]\tvalidation_0-mlogloss:0.93595\tvalidation_0-macroF1:0.55984\n",
      "[200]\tvalidation_0-mlogloss:0.93333\tvalidation_0-macroF1:0.56472\n",
      "[250]\tvalidation_0-mlogloss:0.93272\tvalidation_0-macroF1:0.56666\n",
      "[299]\tvalidation_0-mlogloss:0.93658\tvalidation_0-macroF1:0.56512\n",
      "Train F1: 0.9191548679980808\n",
      "Test F1: 0.4410584092468862\n",
      "[11:25:10] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:25:10] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30819\tvalidation_0-macroF1:0.65153\n",
      "[50]\tvalidation_0-mlogloss:0.94081\tvalidation_0-macroF1:0.58097\n",
      "[100]\tvalidation_0-mlogloss:0.93898\tvalidation_0-macroF1:0.58305\n",
      "[150]\tvalidation_0-mlogloss:0.93914\tvalidation_0-macroF1:0.57999\n",
      "[200]\tvalidation_0-mlogloss:0.93861\tvalidation_0-macroF1:0.57995\n",
      "[250]\tvalidation_0-mlogloss:0.93986\tvalidation_0-macroF1:0.57963\n",
      "[299]\tvalidation_0-mlogloss:0.93986\tvalidation_0-macroF1:0.57343\n",
      "Train F1: 0.9211770353103066\n",
      "Test F1: 0.4265726448132543\n",
      "[11:25:57] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:25:57] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29925\tvalidation_0-macroF1:0.62540\n",
      "[50]\tvalidation_0-mlogloss:0.94068\tvalidation_0-macroF1:0.61427\n",
      "[100]\tvalidation_0-mlogloss:0.94022\tvalidation_0-macroF1:0.61257\n",
      "[150]\tvalidation_0-mlogloss:0.94150\tvalidation_0-macroF1:0.62395\n",
      "[200]\tvalidation_0-mlogloss:0.94161\tvalidation_0-macroF1:0.62662\n",
      "[250]\tvalidation_0-mlogloss:0.94090\tvalidation_0-macroF1:0.62684\n",
      "[299]\tvalidation_0-mlogloss:0.93846\tvalidation_0-macroF1:0.62152\n",
      "Train F1: 0.8914325200424185\n",
      "Test F1: 0.4062737682368002\n",
      "[11:26:45] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:26:45] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30414\tvalidation_0-macroF1:0.62819\n",
      "[50]\tvalidation_0-mlogloss:0.90425\tvalidation_0-macroF1:0.60452\n",
      "[100]\tvalidation_0-mlogloss:0.90078\tvalidation_0-macroF1:0.59850\n",
      "[150]\tvalidation_0-mlogloss:0.89896\tvalidation_0-macroF1:0.59740\n",
      "[200]\tvalidation_0-mlogloss:0.89983\tvalidation_0-macroF1:0.59716\n",
      "[250]\tvalidation_0-mlogloss:0.90116\tvalidation_0-macroF1:0.59179\n",
      "[299]\tvalidation_0-mlogloss:0.89927\tvalidation_0-macroF1:0.59732\n",
      "Train F1: 0.8815909076733524\n",
      "Test F1: 0.41570801903282895\n",
      "[12:42:22] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:42:22] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30156\tvalidation_0-macroF1:0.63965\n",
      "[50]\tvalidation_0-mlogloss:0.93352\tvalidation_0-macroF1:0.63734\n",
      "[100]\tvalidation_0-mlogloss:0.92808\tvalidation_0-macroF1:0.63249\n",
      "[150]\tvalidation_0-mlogloss:0.92255\tvalidation_0-macroF1:0.63234\n",
      "[200]\tvalidation_0-mlogloss:0.92435\tvalidation_0-macroF1:0.62190\n",
      "[250]\tvalidation_0-mlogloss:0.92422\tvalidation_0-macroF1:0.62069\n",
      "[299]\tvalidation_0-mlogloss:0.92578\tvalidation_0-macroF1:0.62153\n",
      "Train F1: 0.9291867495046064\n",
      "Test F1: 0.3793091891188338\n",
      "[12:43:01] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:43:01] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29838\tvalidation_0-macroF1:0.63577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalidation_0-mlogloss:0.94064\tvalidation_0-macroF1:0.60123\n",
      "[100]\tvalidation_0-mlogloss:0.93510\tvalidation_0-macroF1:0.60650\n",
      "[150]\tvalidation_0-mlogloss:0.93318\tvalidation_0-macroF1:0.61361\n",
      "[200]\tvalidation_0-mlogloss:0.93335\tvalidation_0-macroF1:0.61474\n",
      "[250]\tvalidation_0-mlogloss:0.93102\tvalidation_0-macroF1:0.61595\n",
      "[299]\tvalidation_0-mlogloss:0.93041\tvalidation_0-macroF1:0.60532\n",
      "Train F1: 0.8734952402770229\n",
      "Test F1: 0.41661367249602543\n",
      "[12:43:22] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:43:22] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29334\tvalidation_0-macroF1:0.59411\n",
      "[50]\tvalidation_0-mlogloss:0.88988\tvalidation_0-macroF1:0.55837\n",
      "[100]\tvalidation_0-mlogloss:0.88902\tvalidation_0-macroF1:0.56539\n",
      "[150]\tvalidation_0-mlogloss:0.88917\tvalidation_0-macroF1:0.57611\n",
      "[200]\tvalidation_0-mlogloss:0.88982\tvalidation_0-macroF1:0.57083\n",
      "[250]\tvalidation_0-mlogloss:0.88938\tvalidation_0-macroF1:0.58018\n",
      "[299]\tvalidation_0-mlogloss:0.89161\tvalidation_0-macroF1:0.58327\n",
      "Train F1: 0.8862615419142705\n",
      "Test F1: 0.4570355397205492\n",
      "[12:43:44] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:43:44] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29853\tvalidation_0-macroF1:0.63786\n",
      "[50]\tvalidation_0-mlogloss:0.87801\tvalidation_0-macroF1:0.59353\n",
      "[100]\tvalidation_0-mlogloss:0.87355\tvalidation_0-macroF1:0.59615\n",
      "[150]\tvalidation_0-mlogloss:0.87321\tvalidation_0-macroF1:0.58364\n",
      "[200]\tvalidation_0-mlogloss:0.87238\tvalidation_0-macroF1:0.58058\n",
      "[250]\tvalidation_0-mlogloss:0.87407\tvalidation_0-macroF1:0.58709\n",
      "[299]\tvalidation_0-mlogloss:0.87275\tvalidation_0-macroF1:0.59233\n",
      "Train F1: 0.885955055722444\n",
      "Test F1: 0.4315936922032494\n",
      "[12:44:04] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:44:04] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30333\tvalidation_0-macroF1:0.66678\n",
      "[50]\tvalidation_0-mlogloss:0.93317\tvalidation_0-macroF1:0.57606\n",
      "[100]\tvalidation_0-mlogloss:0.93219\tvalidation_0-macroF1:0.56235\n",
      "[150]\tvalidation_0-mlogloss:0.92989\tvalidation_0-macroF1:0.56767\n",
      "[200]\tvalidation_0-mlogloss:0.93132\tvalidation_0-macroF1:0.56437\n",
      "[250]\tvalidation_0-mlogloss:0.93340\tvalidation_0-macroF1:0.57546\n",
      "[299]\tvalidation_0-mlogloss:0.93367\tvalidation_0-macroF1:0.57356\n",
      "Train F1: 0.9220447070100107\n",
      "Test F1: 0.4454881987317728\n",
      "[12:44:24] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:44:25] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30378\tvalidation_0-macroF1:0.61845\n",
      "[50]\tvalidation_0-mlogloss:0.93849\tvalidation_0-macroF1:0.58805\n",
      "[100]\tvalidation_0-mlogloss:0.94074\tvalidation_0-macroF1:0.58551\n",
      "[150]\tvalidation_0-mlogloss:0.93832\tvalidation_0-macroF1:0.59201\n",
      "[200]\tvalidation_0-mlogloss:0.93713\tvalidation_0-macroF1:0.58232\n",
      "[250]\tvalidation_0-mlogloss:0.93853\tvalidation_0-macroF1:0.58919\n",
      "[299]\tvalidation_0-mlogloss:0.93904\tvalidation_0-macroF1:0.59089\n",
      "Train F1: 0.9234021590986095\n",
      "Test F1: 0.4200767543859649\n",
      "[12:44:45] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:44:45] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29898\tvalidation_0-macroF1:0.61960\n",
      "[50]\tvalidation_0-mlogloss:0.92283\tvalidation_0-macroF1:0.58381\n",
      "[100]\tvalidation_0-mlogloss:0.91692\tvalidation_0-macroF1:0.59255\n",
      "[150]\tvalidation_0-mlogloss:0.91689\tvalidation_0-macroF1:0.59773\n",
      "[200]\tvalidation_0-mlogloss:0.91562\tvalidation_0-macroF1:0.59918\n",
      "[250]\tvalidation_0-mlogloss:0.91431\tvalidation_0-macroF1:0.59997\n",
      "[299]\tvalidation_0-mlogloss:0.91437\tvalidation_0-macroF1:0.59945\n",
      "Train F1: 0.8456374086213386\n",
      "Test F1: 0.42344825244768136\n",
      "[12:45:05] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:45:05] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.31157\tvalidation_0-macroF1:0.64035\n",
      "[50]\tvalidation_0-mlogloss:0.91755\tvalidation_0-macroF1:0.57954\n",
      "[100]\tvalidation_0-mlogloss:0.90932\tvalidation_0-macroF1:0.58876\n",
      "[150]\tvalidation_0-mlogloss:0.90831\tvalidation_0-macroF1:0.58735\n",
      "[200]\tvalidation_0-mlogloss:0.90766\tvalidation_0-macroF1:0.58569\n",
      "[250]\tvalidation_0-mlogloss:0.90703\tvalidation_0-macroF1:0.58120\n",
      "[299]\tvalidation_0-mlogloss:0.90693\tvalidation_0-macroF1:0.58207\n",
      "Train F1: 0.8909222222351588\n",
      "Test F1: 0.42650874490856716\n",
      "[12:45:25] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:45:26] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29892\tvalidation_0-macroF1:0.61794\n",
      "[50]\tvalidation_0-mlogloss:0.89128\tvalidation_0-macroF1:0.60735\n",
      "[100]\tvalidation_0-mlogloss:0.88423\tvalidation_0-macroF1:0.60024\n",
      "[150]\tvalidation_0-mlogloss:0.88253\tvalidation_0-macroF1:0.59384\n",
      "[200]\tvalidation_0-mlogloss:0.88157\tvalidation_0-macroF1:0.60040\n",
      "[250]\tvalidation_0-mlogloss:0.87968\tvalidation_0-macroF1:0.59975\n",
      "[299]\tvalidation_0-mlogloss:0.87942\tvalidation_0-macroF1:0.59316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.8643013614940014\n",
      "Test F1: 0.41570324854853846\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VotingClassifierLGBM' object has no attribute 'estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12484/774893938.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m            sample_weight=y_train_weights, threshold=False, **fit_params)\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mclf_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'VotingClassifierLGBM' object has no attribute 'estimator_'"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "for i in range(15):\n",
    "    clf = xgb.XGBClassifier(random_state=217+i, n_estimators=300, \n",
    "                            learning_rate=0.15, n_jobs=4, **opt_parameters)\n",
    "    clfs.append(('xgb{}'.format(i), clf))\n",
    "    \n",
    "vc = VotingClassifierLGBM(clfs, voting='soft')\n",
    "del clfs\n",
    "\n",
    "_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1), y_train, \n",
    "           sample_weight=y_train_weights, threshold=False, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eeba8163",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_final = vc.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "687174be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a single LGBM Classifier: 0.7998\n",
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.9170\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.9126\n"
     ]
    }
   ],
   "source": [
    "global_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "vc.voting = 'soft'\n",
    "global_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "vc.voting = 'hard'\n",
    "global_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d43bfa8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agg18_estadocivil4_COUNT',\n",
       " 'agg18_estadocivil5_COUNT',\n",
       " 'geo_energcocinar_LE_0',\n",
       " 'geo_epared_LE_2'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "for est in vc.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c505a978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 57 (0.023220) - agg18_escolari_MAX\n",
      "2. feature 37 (0.018244) - SQBedjefe\n",
      "3. feature 38 (0.016729) - SQBhogar_nin\n",
      "4. feature 17 (0.015360) - male\n",
      "5. feature 110 (0.014788) - geo_etecho_LE_1\n",
      "6. feature 22 (0.013387) - dependency\n",
      "7. feature 34 (0.012110) - SQBescolari\n",
      "8. feature 58 (0.011792) - agg18_escolari_MEAN\n",
      "9. feature 122 (0.011526) - geo_sanitario_LE_1\n",
      "10. feature 4 (0.011218) - refrig\n",
      "11. feature 72 (0.011116) - agg18_parentesco2_MEAN\n",
      "12. feature 92 (0.010872) - etecho_LE\n",
      "13. feature 9 (0.010793) - r4m1\n",
      "14. feature 15 (0.010598) - cielorazo\n",
      "15. feature 40 (0.010596) - SQBdependency\n",
      "16. feature 91 (0.010523) - epared_LE\n",
      "17. feature 123 (0.010517) - geo_sanitario_LE_2\n",
      "18. feature 61 (0.010436) - agg18_estadocivil2_MEAN\n",
      "19. feature 6 (0.010413) - r4h1\n",
      "20. feature 107 (0.010381) - geo_eviv_LE_1\n",
      "21. feature 114 (0.010081) - geo_elimbasu_LE_0\n",
      "22. feature 104 (0.009962) - geo_bedrooms\n",
      "23. feature 19 (0.009939) - hogar_adul\n",
      "24. feature 41 (0.009916) - SQBmeaned\n",
      "25. feature 125 (0.009902) - geo_sanitario_LE_4\n",
      "26. feature 93 (0.009781) - eviv_LE\n",
      "27. feature 11 (0.009754) - r4m3\n",
      "28. feature 96 (0.009730) - tipovivi_LE\n",
      "29. feature 49 (0.009666) - fe_mobile_density\n",
      "30. feature 102 (0.009592) - geo_hogar_adul\n",
      "31. feature 94 (0.009439) - estadocivil_LE\n",
      "32. feature 13 (0.009373) - r4t2\n",
      "33. feature 12 (0.009273) - r4t1\n",
      "34. feature 99 (0.009220) - geo_meaneduc\n",
      "35. feature 25 (0.009038) - meaneduc\n",
      "36. feature 27 (0.008949) - overcrowding\n",
      "37. feature 56 (0.008918) - agg18_escolari_MIN\n",
      "38. feature 70 (0.008862) - agg18_estadocivil7_MEAN\n",
      "39. feature 39 (0.008860) - SQBovercrowding\n",
      "40. feature 103 (0.008853) - geo_hogar_total\n",
      "41. feature 14 (0.008844) - escolari\n",
      "42. feature 67 (0.008773) - agg18_estadocivil5_MEAN\n",
      "43. feature 33 (0.008753) - age\n",
      "44. feature 85 (0.008684) - piso_LE\n",
      "45. feature 51 (0.008684) - fe_mobile_adult_density\n",
      "46. feature 120 (0.008647) - geo_energcocinar_LE_3\n",
      "47. feature 63 (0.008593) - agg18_estadocivil3_MEAN\n",
      "48. feature 54 (0.008574) - agg18_age_MAX\n",
      "49. feature 69 (0.008543) - agg18_estadocivil6_MEAN\n",
      "50. feature 84 (0.008463) - pared_LE\n",
      "51. feature 26 (0.008454) - bedrooms\n",
      "52. feature 132 (0.008395) - geo_pared_LE_2\n",
      "53. feature 105 (0.008375) - geo_overcrowding\n",
      "54. feature 100 (0.008287) - geo_dependency\n",
      "55. feature 95 (0.008200) - lugar_LE\n",
      "56. feature 45 (0.008158) - fe_human_density\n",
      "57. feature 98 (0.008142) - geo_age\n",
      "58. feature 30 (0.008110) - qmobilephone\n",
      "59. feature 21 (0.008069) - hogar_total\n",
      "60. feature 126 (0.008063) - geo_manual_elec_LE_0\n",
      "61. feature 115 (0.008033) - geo_elimbasu_LE_1\n",
      "62. feature 2 (0.008014) - rooms\n",
      "63. feature 53 (0.007955) - agg18_age_MIN\n",
      "64. feature 35 (0.007903) - SQBage\n",
      "65. feature 129 (0.007828) - geo_manual_elec_LE_4\n",
      "66. feature 46 (0.007688) - fe_human_bed_density\n",
      "67. feature 90 (0.007661) - elimbasu_LE\n",
      "68. feature 7 (0.007572) - r4h2\n",
      "69. feature 23 (0.007557) - edjefe\n",
      "70. feature 141 (0.007492) - rent_to_hhsize\n",
      "71. feature 59 (0.007366) - agg18_dis_MEAN\n",
      "72. feature 134 (0.007351) - bedrooms_to_rooms\n",
      "73. feature 43 (0.007305) - fe_working_man_fraction\n",
      "74. feature 31 (0.007296) - area1\n",
      "75. feature 44 (0.007288) - fe_all_man_fraction\n",
      "76. feature 118 (0.007264) - geo_elimbasu_LE_5\n",
      "77. feature 89 (0.007219) - energcocinar_LE\n",
      "78. feature 5 (0.007150) - v18q1\n",
      "79. feature 111 (0.007100) - geo_etecho_LE_2\n",
      "80. feature 52 (0.007035) - fe_tablet_adult_density\n",
      "81. feature 47 (0.007013) - fe_rent_per_person\n",
      "82. feature 8 (0.006868) - r4h3\n",
      "83. feature 136 (0.006866) - tamhog_to_rooms\n",
      "84. feature 55 (0.006863) - agg18_age_MEAN\n",
      "85. feature 24 (0.006862) - edjefa\n",
      "86. feature 60 (0.006851) - agg18_estadocivil1_COUNT\n",
      "87. feature 0 (0.006781) - v2a1\n",
      "88. feature 36 (0.006690) - SQBhogar_total\n",
      "89. feature 138 (0.006610) - r4t3_to_rooms\n",
      "90. feature 50 (0.006556) - fe_tablet_density\n",
      "91. feature 88 (0.006504) - sanitario_LE\n",
      "92. feature 10 (0.006494) - r4m2\n",
      "93. feature 3 (0.006461) - hacapo\n",
      "94. feature 65 (0.006434) - agg18_estadocivil4_MEAN\n",
      "95. feature 16 (0.006415) - dis\n",
      "96. feature 97 (0.006369) - manual_elec_LE\n",
      "97. feature 42 (0.006320) - fe_children_fraction\n",
      "98. feature 18 (0.006212) - hogar_nin\n",
      "99. feature 48 (0.006049) - fe_rent_per_room\n",
      "100. feature 108 (0.006031) - geo_eviv_LE_2\n",
      "101. feature 29 (0.005597) - television\n",
      "102. feature 79 (0.005522) - agg18_parentesco9_MEAN\n",
      "103. feature 135 (0.005495) - rent_to_rooms\n",
      "104. feature 86 (0.005298) - techo_LE\n",
      "105. feature 28 (0.005282) - computer\n",
      "106. feature 73 (0.005197) - agg18_parentesco3_MEAN\n",
      "107. feature 139 (0.004915) - v2a1_to_r4t3\n",
      "108. feature 101 (0.004888) - geo_hogar_nin\n",
      "109. feature 20 (0.004462) - hogar_mayor\n",
      "110. feature 76 (0.004369) - agg18_parentesco6_MEAN\n",
      "111. feature 112 (0.004336) - geo_epared_LE_1\n",
      "112. feature 77 (0.004260) - agg18_parentesco7_MEAN\n",
      "113. feature 32 (0.004221) - area2\n",
      "114. feature 1 (0.004084) - hacdor\n",
      "115. feature 142 (0.004019) - rent_to_over_18\n",
      "116. feature 75 (0.003787) - agg18_parentesco5_MEAN\n",
      "117. feature 81 (0.003761) - agg18_parentesco11_MEAN\n",
      "118. feature 140 (0.003638) - hhsize_to_rooms\n",
      "119. feature 83 (0.003287) - edjef\n",
      "120. feature 74 (0.003173) - agg18_parentesco4_MEAN\n",
      "121. feature 82 (0.003171) - agg18_parentesco12_MEAN\n",
      "122. feature 87 (0.003151) - abastagua_LE\n",
      "123. feature 124 (0.002927) - geo_sanitario_LE_3\n",
      "124. feature 62 (0.002358) - agg18_estadocivil2_COUNT\n",
      "125. feature 80 (0.002208) - agg18_parentesco10_MEAN\n",
      "126. feature 78 (0.001442) - agg18_parentesco8_MEAN\n",
      "127. feature 137 (0.001289) - r4t3_to_tamhog\n",
      "128. feature 121 (0.000000) - geo_sanitario_LE_0\n",
      "129. feature 133 (0.000000) - geo_pared_LE_7\n",
      "130. feature 131 (0.000000) - geo_pared_LE_1\n",
      "131. feature 130 (0.000000) - geo_pared_LE_0\n",
      "132. feature 128 (0.000000) - geo_manual_elec_LE_3\n",
      "133. feature 127 (0.000000) - geo_manual_elec_LE_1\n",
      "134. feature 113 (0.000000) - geo_epared_LE_2\n",
      "135. feature 119 (0.000000) - geo_energcocinar_LE_0\n",
      "136. feature 117 (0.000000) - geo_elimbasu_LE_3\n",
      "137. feature 116 (0.000000) - geo_elimbasu_LE_2\n",
      "138. feature 64 (0.000000) - agg18_estadocivil3_COUNT\n",
      "139. feature 109 (0.000000) - geo_etecho_LE_0\n",
      "140. feature 106 (0.000000) - geo_eviv_LE_0\n",
      "141. feature 68 (0.000000) - agg18_estadocivil5_COUNT\n",
      "142. feature 66 (0.000000) - agg18_estadocivil4_COUNT\n",
      "143. feature 71 (0.000000) - agg18_parentesco1_MEAN\n"
     ]
    }
   ],
   "source": [
    "ranked_features = feature_importance(clf_final, X_train.drop(xgb_drop_cols, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578be04c",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2f52fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n",
    "       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n",
    "       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n",
    "       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n",
    "       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n",
    "       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n",
    "       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n",
    "       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n",
    "       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n",
    "       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n",
    "       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n",
    "       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n",
    "       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n",
    "       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n",
    "       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n",
    "       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n",
    "       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n",
    "       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n",
    "       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n",
    "       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n",
    "       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN']\n",
    "\n",
    "et_drop_cols.extend([\"idhogar\", \"parentesco1\", 'fe_rent_per_person', 'fe_rent_per_room',\n",
    "       'fe_tablet_adult_density', 'fe_tablet_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cce9592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.8979027907232522\n",
      "Test F1: 0.40019607791540907\n",
      "Train F1: 0.8915835165085447\n",
      "Test F1: 0.41728706352222\n",
      "Train F1: 0.8872408153380176\n",
      "Test F1: 0.38255725717439293\n",
      "Train F1: 0.8923733328819934\n",
      "Test F1: 0.42440028340320635\n",
      "Train F1: 0.8937744901800107\n",
      "Test F1: 0.4473473017722438\n",
      "Train F1: 0.8908301334344013\n",
      "Test F1: 0.42197562528872723\n",
      "Train F1: 0.8898659393321041\n",
      "Test F1: 0.42275712280957956\n",
      "Train F1: 0.8936434694386128\n",
      "Test F1: 0.4789151810379117\n",
      "Train F1: 0.9040383050165901\n",
      "Test F1: 0.4390145892884785\n",
      "Train F1: 0.8919137395147149\n",
      "Test F1: 0.4166328825500241\n"
     ]
    }
   ],
   "source": [
    "ets = []\n",
    "for i in range(10):\n",
    "    rf = RandomForestClassifier(\n",
    "        max_depth=None, random_state=217+i, n_jobs=4, n_estimators=700, \n",
    "        min_impurity_decrease=1e-3, min_samples_leaf=2, verbose=0, \n",
    "        class_weight='balanced')\n",
    "    ets.append(('rf{}'.format(i), rf))\n",
    "\n",
    "vc2 = VotingClassifierLGBM(ets, voting='soft')\n",
    "_ = vc2.fit(X_train.drop(et_drop_cols, axis=1), y_train, threshold=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4333a71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a VotingClassifier on 3 RFs with soft voting strategy: 0.8951\n",
      "Validation score of a VotingClassifier on 3 RFs with hard voting strategy: 0.9122\n"
     ]
    }
   ],
   "source": [
    "vc2.voting = 'soft'\n",
    "global_rf_score_soft = f1_score(\n",
    "    y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "vc2.voting = 'hard'\n",
    "global_rf_score_hard = f1_score(\n",
    "    y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 RFs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 RFs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a19f4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parentesco_LE', 'rez_esc'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "for est in vc2.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(\n",
    "        est, X_train.drop(et_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35254f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_voters(data, weights=[0.5, 0.5]):\n",
    "    vc.voting='soft'\n",
    "    vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols, axis=1))\n",
    "    vc2.voting='soft'\n",
    "    vc2_probs = vc2.predict_proba(data.drop(et_drop_cols, axis=1))\n",
    "    \n",
    "    final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1])\n",
    "    predictions = np.argmax(final_vote, axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03513ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9188277400731601"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.5, 0.5])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a3a7cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9163348691684967"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.6, 0.4])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
