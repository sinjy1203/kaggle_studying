{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4e66df",
   "metadata": {},
   "source": [
    "# Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cbb267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['statoil-iceberg-classifier-challenge',\n",
       " 'statoil-iceberg-submissions',\n",
       " 'submission38-lb01448']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "root_dir = Path('C:/Users/sinjy/jupyter_notebook/datasets')\n",
    "data_dir = root_dir / 'kaggle_datasets' / 'Iceberg'\n",
    "predict_dir = root_dir / 'kaggle_predict'\n",
    "\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d09c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sub_200_ens_densenet.csv',\n",
       " 'sub_blend009.csv',\n",
       " 'sub_fcn.csv',\n",
       " 'sub_keras_beginner.csv',\n",
       " 'sub_TF_keras.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir / 'statoil-iceberg-submissions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7d9a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['submission38.csv', 'submission43.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir / 'submission38-lb01448')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b79d54",
   "metadata": {},
   "source": [
    "## pytorch CNN DenseNet Ensemble\n",
    "## ==> sub_200_ens_densenet.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfdc85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import logging\n",
    "import psutil\n",
    "import os\n",
    "import scipy.signal\n",
    "import random\n",
    "from datetime import datetime\n",
    "from scipy import signal\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd346654",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b829fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a11d425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]\n",
      "10.6\n",
      "svmem(total=16980230144, available=11840733184, percent=30.3, used=5139496960, free=11840733184)\n",
      "memory GB: 0.19475173950195312\n"
     ]
    }
   ],
   "source": [
    "def cpuStats():\n",
    "    print(sys.version) ## python version\n",
    "    print(psutil.cpu_percent())  ## present cpu utilization rate\n",
    "    print(psutil.virtual_memory())  ## memory\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0] / 2. ** 30\n",
    "    print('memory GB:', memoryUse)\n",
    "    \n",
    "cpuStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db9c3fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:USE CUDA=True\n"
     ]
    }
   ],
   "source": [
    "lgr.info(\"USE CUDA=\" + str(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90eb4ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 17 * 19\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd4ce6",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98bcd3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VAR = 'target'\n",
    "BASE_FOLDER = data_dir / 'statoil-iceberg-classifier-challenge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3fbdad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 5)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json(BASE_FOLDER / 'train.json')\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d107508f",
   "metadata": {},
   "source": [
    "### shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d0b5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(datetime.now())\n",
    "data = shuffle(data)\n",
    "data = data.reindex(np.random.permutation(data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20f57095",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['band_1'] = data['band_1'].apply(lambda x: np.array(x).reshape(75,75))\n",
    "data['band_2'] = data['band_2'].apply(lambda x: np.array(x).reshape(75,75))\n",
    "\n",
    "data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2019c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_1 = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "band_2 = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "full_img = np.stack([band_1, band_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3978acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XnumpyToTensor(x_data_np):\n",
    "    x_data_np = np.array(x_data_np, dtype=np.float32)\n",
    "    print(x_data_np.shape)\n",
    "    print(type(x_data_np))\n",
    "    \n",
    "    if use_cuda:\n",
    "        lgr.info(\"Using the GPU\")\n",
    "        X_tensor = (torch.from_numpy(x_data_np).cuda())\n",
    "    else:\n",
    "        lgr.info(\"Using the CPU\")\n",
    "        X_tensor = (torch.from_numpy(x_data_np))\n",
    "        \n",
    "    print((X_tensor.shape))\n",
    "    return X_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8eaab7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def YnumpyToTensor(y_data_np):\n",
    "    y_data_np = y_data_np.reshape((y_data_np.shape[0], 1))\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "    \n",
    "    if use_cuda:\n",
    "        lgr.info(\"Using the GPU\")\n",
    "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()\n",
    "    else:\n",
    "        lgr.info(\"Using the CPU\")\n",
    "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor)\n",
    "        \n",
    "    print(type(Y_tensor))\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "    return Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6673cd",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10e88635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTrainingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, full_ds, offset, length):\n",
    "        self.full_ds = full_ds\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(full_ds) >= offset + length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(FullTrainingDataset, self).__init__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.full_ds[i+self.offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b1b69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "validationRatio = 0.11\n",
    "def trainTestSplit(dataset, val_share=validationRatio):\n",
    "    val_offset = int(len(dataset) * (1 - val_share))\n",
    "    print(\"offset:\", str(val_offset))\n",
    "    return FullTrainingDataset(dataset, 0, val_offset), FullTrainingDataset(dataset, val_offset, len(dataset) - val_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73d9da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 2, 75, 75)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1604, 2, 75, 75])\n",
      "(1604, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n",
      "(1604, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_imgs = XnumpyToTensor(full_img)\n",
    "train_targets = YnumpyToTensor(data['is_iceberg'].values)\n",
    "dset_train = TensorDataset(train_imgs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d0f1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset: 1427\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E88C14DCC8>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E894595D88>\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds = trainTestSplit(dset_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, \n",
    "                                          shuffle=False, num_workers=1)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, \n",
    "                                        shuffle=False, num_workers=1)\n",
    "print(val_loader)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed208cdb",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56cc500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "n_channels = 2\n",
    "total_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf84b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4 * growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1, \n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3, \n",
    "                              padding=1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1db7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3, padding=1,\n",
    "                              bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be055d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1, \n",
    "                              bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd406395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "        \n",
    "        nChannels = 2 * growthRate\n",
    "        self.conv1 = nn.Conv2d(2, nChannels, kernel_size=3, padding=1, \n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, \n",
    "                                      bottleneck)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        nOutChannels = int(math.floor(nChannels * reduction))\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "        \n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, \n",
    "                                      bottleneck)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        nOutChannels = int(math.floor(nChannels * reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "        \n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_sense(nChannels, growthRate, nDenseBlocks, \n",
    "                                      bottleneck)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(128, nClasses)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn1(out)), 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.sigmoid(self.fc(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78730a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
