{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4e66df",
   "metadata": {},
   "source": [
    "# Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cbb267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'statoil-iceberg-classifier-challenge',\n",
       " 'statoil-iceberg-submissions',\n",
       " 'submission38-lb01448']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "root_dir = Path('C:/Users/sinjy/jupyter_notebook/datasets')\n",
    "data_dir = root_dir / 'kaggle_datasets' / 'Iceberg'\n",
    "predict_dir = root_dir / 'kaggle_predict'\n",
    "\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d09c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sub_200_ens_densenet.csv',\n",
       " 'sub_blend009.csv',\n",
       " 'sub_fcn.csv',\n",
       " 'sub_keras_beginner.csv',\n",
       " 'sub_TF_keras.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir / 'statoil-iceberg-submissions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7d9a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['submission38.csv', 'submission43.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir / 'submission38-lb01448')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b79d54",
   "metadata": {},
   "source": [
    "## pytorch CNN DenseNet Ensemble\n",
    "## ==> sub_200_ens_densenet.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfdc85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import logging\n",
    "import psutil\n",
    "import os\n",
    "import scipy.signal\n",
    "import random\n",
    "from datetime import datetime\n",
    "from scipy import signal\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd346654",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b829fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a11d425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]\n",
      "21.7\n",
      "svmem(total=16980230144, available=11203145728, percent=34.0, used=5777084416, free=11203145728)\n",
      "memory GB: 0.19483566284179688\n"
     ]
    }
   ],
   "source": [
    "def cpuStats():\n",
    "    print(sys.version) ## python version\n",
    "    print(psutil.cpu_percent())  ## present cpu utilization rate\n",
    "    print(psutil.virtual_memory())  ## memory\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0] / 2. ** 30\n",
    "    print('memory GB:', memoryUse)\n",
    "    \n",
    "cpuStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db9c3fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:USE CUDA=True\n"
     ]
    }
   ],
   "source": [
    "lgr.info(\"USE CUDA=\" + str(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90eb4ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 17 * 19\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd4ce6",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98bcd3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VAR = 'target'\n",
    "BASE_FOLDER = data_dir / 'statoil-iceberg-classifier-challenge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3fbdad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 5)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json(BASE_FOLDER / 'train.json')\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d107508f",
   "metadata": {},
   "source": [
    "### shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d0b5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(datetime.now())\n",
    "data = shuffle(data)\n",
    "data = data.reindex(np.random.permutation(data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20f57095",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['band_1'] = data['band_1'].apply(lambda x: np.array(x).reshape(75,75))\n",
    "data['band_2'] = data['band_2'].apply(lambda x: np.array(x).reshape(75,75))\n",
    "\n",
    "data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2019c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_1 = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "band_2 = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "full_img = np.stack([band_1, band_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3978acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XnumpyToTensor(x_data_np):\n",
    "    x_data_np = np.array(x_data_np, dtype=np.float32)\n",
    "    print(x_data_np.shape)\n",
    "    print(type(x_data_np))\n",
    "    \n",
    "    if use_cuda:\n",
    "        lgr.info(\"Using the GPU\")\n",
    "        X_tensor = (torch.from_numpy(x_data_np).cuda())\n",
    "    else:\n",
    "        lgr.info(\"Using the CPU\")\n",
    "        X_tensor = (torch.from_numpy(x_data_np))\n",
    "        \n",
    "    print((X_tensor.shape))\n",
    "    return X_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eaab7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def YnumpyToTensor(y_data_np):\n",
    "    y_data_np = y_data_np.reshape((y_data_np.shape[0], 1))\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "    \n",
    "    if use_cuda:\n",
    "        lgr.info(\"Using the GPU\")\n",
    "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()\n",
    "    else:\n",
    "        lgr.info(\"Using the CPU\")\n",
    "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor)\n",
    "        \n",
    "    print(type(Y_tensor))\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "    return Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6673cd",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10e88635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTrainingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, full_ds, offset, length):\n",
    "        self.full_ds = full_ds\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(full_ds) >= offset + length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(FullTrainingDataset, self).__init__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.full_ds[i+self.offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b1b69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "validationRatio = 0.11\n",
    "def trainTestSplit(dataset, val_share=validationRatio):\n",
    "    val_offset = int(len(dataset) * (1 - val_share))\n",
    "    print(\"offset:\", str(val_offset))\n",
    "    return FullTrainingDataset(dataset, 0, val_offset), FullTrainingDataset(dataset, val_offset, len(dataset) - val_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73d9da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 2, 75, 75)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1604, 2, 75, 75])\n",
      "(1604, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n",
      "(1604, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_imgs = XnumpyToTensor(full_img)\n",
    "train_targets = YnumpyToTensor(data['is_iceberg'].values)\n",
    "dset_train = TensorDataset(train_imgs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d0f1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset: 1427\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001B33AE9E508>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001B33AE9EF08>\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds = trainTestSplit(dset_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, \n",
    "                                        shuffle=False)\n",
    "print(val_loader)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed208cdb",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56cc500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "n_channels = 2\n",
    "total_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf84b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4 * growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1, \n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3, \n",
    "                              padding=1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1db7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3, padding=1,\n",
    "                              bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be055d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1, \n",
    "                              bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd406395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "        \n",
    "        nChannels = 2 * growthRate\n",
    "        self.conv1 = nn.Conv2d(2, nChannels, kernel_size=3, padding=1, \n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, \n",
    "                                      bottleneck)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        nOutChannels = int(math.floor(nChannels * reduction))\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "        \n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, \n",
    "                                      bottleneck)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        nOutChannels = int(math.floor(nChannels * reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "        \n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, \n",
    "                                      bottleneck)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(128, nClasses)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn1(out)), 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.sigmoid(self.fc(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78730a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet(growthRate=8, depth=20, reduction=0.5, bottleneck=True, \n",
    "                nClasses=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8dadd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Number of params: 19921\n"
     ]
    }
   ],
   "source": [
    "print('+ Number of params: {}'.format(sum([p.data.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99e9cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (conv1): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (dense1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(24, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (trans1): Transition(\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (dense2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(24, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (trans2): Transition(\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (dense3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(24, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec976aeb",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c68aaff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n",
      "INFO:__main__:Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 5e-05\n",
      ")\n",
      "INFO:__main__:BCELoss()\n"
     ]
    }
   ],
   "source": [
    "loss_func = torch.nn.BCELoss()\n",
    "\n",
    "LR = 0.0005\n",
    "MOMENTUM = 0.95\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=5e-5)\n",
    "if use_cuda:\n",
    "    lgr.info(\"Using the GPU\")\n",
    "    model.cuda()\n",
    "    loss_func.cuda()\n",
    "lgr.info(optimizer)\n",
    "lgr.info(loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880465f3",
   "metadata": {},
   "source": [
    "### this model just example \n",
    "### which take few days of training gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f15ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# criterion = loss_func\n",
    "# all_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "#         print('*' * 5 + ':')\n",
    "#         running_loss = 0.0\n",
    "#         running_acc = 0.0\n",
    "#         for i, data in enumerate(train_loader, 1):\n",
    "#             img, label = data\n",
    "#             if use_cuda:\n",
    "#                 img, label = img.cuda(non_blocking=True), label.cuda(non_blocking=True)\n",
    "                \n",
    "#             out = model(img)\n",
    "#             loss = criterion(out, label)\n",
    "#             running_loss += loss.item() * label.size(0)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             if i % 10 == 0:\n",
    "#                 all_losses.append(running_loss / (batch_size * i))\n",
    "#                 print('[{}/{}] Loss: {:.6f}'.format(epoch+1, num_epochs, running_loss / (batch_size * i), \n",
    "#                                                    running_acc / (batch_size * i)))\n",
    "#         print('Finish {} epoch, Loss: {:.6f}'.format(epoch+1, running_loss / (len(train_ds))))\n",
    "        \n",
    "#         model.eval()\n",
    "#         eval_loss = 0\n",
    "#         eval_acc = 0\n",
    "#         with torch.no_grad():\n",
    "#             for data in val_loader:\n",
    "#                 img, label = data\n",
    "\n",
    "#                 if use_cuda:\n",
    "#                     img, label = img.cuda(non_blocking=True), label.cuda(non_blocking=True)\n",
    "#                 out = model(img)\n",
    "#                 loss = criterion(out, label)\n",
    "#                 eval_loss += loss.item() * label.size(0)\n",
    "#         print('VALIDATION Loss: {:.6f}'.format(eval_loss / (len(val_ds))))\n",
    "#         val_losses.append(eval_loss / len(val_ds))\n",
    "#         print()\n",
    "    \n",
    "#     torch.save(model.state_dict(), data_dir / 'model' / '200_ens_densenet.pth')\n",
    "    \n",
    "#     df_test_set = pd.read_json(BASE_FOLDER / 'test.json')\n",
    "    \n",
    "#     df_test_set['band_1'] = df_test_set['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "#     df_test_set['band_2'] = df_test_set['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "#     df_test_set['inc_angle'] = pd.to_numeric(df_test_set['inc_angle'], errors='coerce')\n",
    "    \n",
    "#     columns = ['id', 'is_iceberg']\n",
    "#     df_pred = pd.DataFrame(data=np.zeros((0, len(columns))), columns=columns)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for index, row in df_test_set.iterrows():\n",
    "#             rwo_no_id = row.drop('id')\n",
    "#             band_1_test = (rwo_no_id['band_1']).reshape(-1, 75, 75)\n",
    "#             band_2_test = (rwo_no_id['band_2']).reshape(-1, 75, 75)\n",
    "#             full_img_test = np.stack([band_1_test, band_2_test], axis=1)\n",
    "\n",
    "#             x_data_np = np.array(full_img_test, dtype=np.float32)\n",
    "#             if use_cuda:\n",
    "#                 X_tensor_test = torch.from_numpy(x_data_np).cuda()\n",
    "#             else:\n",
    "#                 X_tensor_test = torch.from_numpy(x_data_np)\n",
    "            \n",
    "#             predicted_val = (model(X_tensor_test).item())\n",
    "#             df_pred = df_pred.append({'id': row['id'], 'is_iceberg': predicted_val}, ignore_index=True)\n",
    "    \n",
    "#     def savePred(df_pred):\n",
    "#         csv_path = predict_dir / 'sub_200_ens_densenet.csv'\n",
    "#         df_pred.to_csv(csv_path, columns=('id', 'is_iceberg'), index=None)\n",
    "        \n",
    "#     savePred(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34c30d",
   "metadata": {},
   "source": [
    "### test score: 0.15381"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
